This is a Wikipedia sort-of-bot for maintaining a central store of language classification data for use in articles.

(NB: the page names used are currently in userspace/sandboxes while I shake down any kinks left in it.)

The canonical place for classification data is on subpages of [[User:KleptomaniacViolet/Language families data/]] (e.g., [[User:KleptomaniacViolet/Language families data/Dravidian languages]]). These are a tree-based format using lists and templates to structure the data while staying human-friendly. All subpages will be gone over by the bot. Note that large trees can be broken across multiple pages, so long as the common node is an article and it's at the root of the 'child' tree and it's a leaf of the 'parent'.

First, the bot reads [[User:KleptomaniacViolet/Language families cookies]]. If that page's text equals "LOCKED", it means that another instance of the bot is currently mid-update and so this cannot safely continue. Otherwise the page's data is parsed and replaced with "LOCKED". The bot will deconstruct the canonical trees into (parent node, child node) relations, and then dump that information into Lua tables located in subpages of [[Module:Sandbox/KleptomaniacViolet/Language families/Data/]]. Large input may cause a naive implementation to go over the 2MB page size limit, so the actual tables are chunked and stored on subsubpages. To preserve atomicity, the bot alternates between writing to an 'alpha' and a 'beta' set of subpages, giving a final page title that looks something like, e.g., [[Module:Sandbox/KleptomaniacViolet/Language families/Data/Dravidian languages/alpha/0]]. The record of which subpage set has which set live has to be stored on-wiki so that separate instances of the bot do not accidentally start uploading data to the live set. [[Module:Sandbox/KleptomaniacViolet/Language families/Data]] is written with the current set of pages with tables of node data in them. Finally [[User:KleptomaniacViolet/Language families cookies]] is written with the list of which subpages are 'alpha' and which are 'beta'.

The unique part of the source subpage's title is preserved in the object subpages' titles so that changes have good locality and do not needlessly cause the entire multi-megabyte node table complex to be reuploaded every time a single input page changes.

Operation
=========

0. Install the pywikibot framework (specifically, the core branch), unicodecsv and mwparserfromhell.

1. Create the operations database. The schema is in a comment in ``data-tables-update.py``.

2. If you're doing dry-runs, create a ``generated`` directory.

3. If you're not doing dry-runs, change ``DRY_RUN`` to ``False`` in ``data-tables-update.py``.

4. ``python data-tables-update.py``.

Miscellaneous notes
===================

Most of the complexity in this code comes from running off-wiki (in particular, the need to maintain atomicity and the chunking). In principle it should be possible to do the heavy lifting of parsing the canonical trees in a Lua module (there aren't that many of them, and it should be fast if enough corners are cut), but writing Lua is much more of an unknown for me than Python, so I have stuck to what I know. If anyone wants to have a go, the main thing will be porting ``lua_table_generate.py`` to Lua itself. A separate list of subpages to go over will have to be maintained, but that has other advantages anyway.
